{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "975286bb-eb22-4b05-a7cc-1be6d61c8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6bb52002-20c8-43fa-83f9-4b1e5790b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 374\n",
      "Sample data:\n",
      "                  description  disease\n",
      "329      Joto linaniua kabisa  Malaria\n",
      "33       Mkojo unanuka vibaya      UTI\n",
      "15   Kutapika na kichefuchefu  Malaria\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('diseases.csv')  # columns: 'description', 'disease'\n",
    "\n",
    "# Verify data\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.sample(3, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03e2384b-74ae-4646-9a40-6ba1d78030b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "def swahili_spelling_variation(word):\n",
    "    \"\"\"Generate common spelling variations for a Swahili word\"\"\"\n",
    "    variations = [word]\n",
    "    \n",
    "    # Common Swahili character-level transformations\n",
    "    transformations = [\n",
    "        # Vowel substitutions\n",
    "        lambda s: s.replace('a', random.choice(['aa', 'e', '']), 1) if 'a' in s else s,\n",
    "        lambda s: s.replace('e', random.choice(['i', 'a', 'ie']), 1) if 'e' in s else s,\n",
    "        lambda s: s.replace('i', random.choice(['e', 'ee', 'y']), 1) if 'i' in s else s,\n",
    "        lambda s: s.replace('u', random.choice(['oo', 'w', 'uu']), 1) if 'u' in s else s,\n",
    "        lambda s: s.replace('o', random.choice(['a', 'ou', 'oo']), 1) if 'o' in s else s,\n",
    "        \n",
    "        # Consonant substitutions\n",
    "        lambda s: s.replace('k', random.choice(['c', 'ch', '']), 1) if 'k' in s else s,\n",
    "        lambda s: s.replace('ch', random.choice(['sh', 'c', 'ky']), 1) if 'ch' in s else s,\n",
    "        lambda s: s.replace('sh', random.choice(['ch', 's', 'sy']), 1) if 'sh' in s else s,\n",
    "        lambda s: s.replace('m', random.choice(['n', 'mb', 'mm']), 1) if 'm' in s else s,\n",
    "        lambda s: s.replace('n', random.choice(['m', 'ny', 'nn']), 1) if 'n' in s else s,\n",
    "        lambda s: s.replace('b', random.choice(['p', 'v', 'bh']), 1) if 'b' in s else s,\n",
    "        lambda s: s.replace('p', random.choice(['b', 'ph', '']), 1) if 'p' in s else s,\n",
    "        lambda s: s.replace('v', random.choice(['b', 'f', 'vh']), 1) if 'v' in s else s,\n",
    "        lambda s: s.replace('f', random.choice(['v', 'p', 'ph']), 1) if 'f' in s else s,\n",
    "        lambda s: s.replace('d', random.choice(['t', 'dh', '']), 1) if 'd' in s else s,\n",
    "        lambda s: s.replace('t', random.choice(['d', 'th', '']), 1) if 't' in s else s,\n",
    "        \n",
    "        # Common Swahili-specific transformations\n",
    "        lambda s: s.replace('ny', random.choice(['n', 'ni', 'gn']), 1) if 'ny' in s else s,\n",
    "        lambda s: s.replace('ng', random.choice(['n', 'g', 'nk']), 1) if 'ng' in s else s,\n",
    "        lambda s: s.replace('gh', random.choice(['g', 'h', '']), 1) if 'gh' in s else s,\n",
    "        lambda s: s.replace('th', random.choice(['t', 's', '']), 1) if 'th' in s else s,\n",
    "        lambda s: s.replace('dh', random.choice(['d', 'z', '']), 1) if 'dh' in s else s,\n",
    "        \n",
    "        # Common Swahili prefix transformations\n",
    "        lambda s: re.sub(r'^(ni)(\\w+)', lambda m: random.choice(['n', 'mi', 'm'])+m.group(2), s) \n",
    "                  if s.startswith('ni') else s,\n",
    "        lambda s: re.sub(r'^(na)(\\w+)', lambda m: random.choice(['n', 'a', 'ma'])+m.group(2), s) \n",
    "                  if s.startswith('na') else s,\n",
    "        lambda s: re.sub(r'^(ku)(\\w+)', lambda m: random.choice(['kw', 'ko', 'k'])+m.group(2), s) \n",
    "                  if s.startswith('ku') else s,\n",
    "        lambda s: re.sub(r'^(ki)(\\w+)', lambda m: random.choice(['ch', 'ky', 'c'])+m.group(2), s) \n",
    "                  if s.startswith('ki') else s,\n",
    "        lambda s: re.sub(r'^(vi)(\\w+)', lambda m: random.choice(['vy', 'v', 'fi'])+m.group(2), s) \n",
    "                  if s.startswith('vi') else s,\n",
    "        \n",
    "        # Random mutations\n",
    "        lambda s: s[:random.randint(1, len(s))] + random.choice(['a', 'e', 'i', 'o', 'u']) + s[random.randint(1, len(s)):] \n",
    "                  if len(s) > 3 else s,\n",
    "        lambda s: s[:random.randint(1, len(s)-1)] + s[random.randint(1, len(s)-1)+1:] \n",
    "                  if len(s) > 3 else s,\n",
    "        lambda s: s[:random.randint(1, len(s)-1)] + random.choice(['h', 'm', 'n']) + s[random.randint(1, len(s)-1):] \n",
    "                  if len(s) > 3 else s,\n",
    "    ]\n",
    "    \n",
    "    # Apply transformations randomly\n",
    "    for _ in range(random.randint(1, 3)):  # Apply 1-3 transformations\n",
    "        if random.random() < 0.7:  # 70% chance to apply a transformation\n",
    "            transform = random.choice(transformations)\n",
    "            new_variant = transform(word)\n",
    "            if new_variant != word:\n",
    "                variations.append(new_variant)\n",
    "    \n",
    "    return list(set(variations))  # Remove duplicates\n",
    "\n",
    "def augment_text(text, num_variations=5):\n",
    "    \"\"\"Generate augmented versions of a text\"\"\"\n",
    "    words = text.split()\n",
    "    augmented_texts = [text]  # Start with original text\n",
    "    \n",
    "    for _ in range(num_variations):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            # Get variations for each word\n",
    "            word_variations = swahili_spelling_variation(word)\n",
    "            # Choose one variation randomly\n",
    "            new_words.append(random.choice(word_variations))\n",
    "        \n",
    "        # Combine words to form new text\n",
    "        new_text = ' '.join(new_words)\n",
    "        if new_text != text:\n",
    "            augmented_texts.append(new_text)\n",
    "    \n",
    "    return list(set(augmented_texts))  # Remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0d13351-7f0d-4a32-9fe8-028480807a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 374\n",
      "Augmented dataset size: 2060\n",
      "Number of variations per sample: 5.5x\n"
     ]
    }
   ],
   "source": [
    "def augment_dataset(df, variations_per_sample=500):\n",
    "    \"\"\"Create augmented dataset with automatic spelling variations\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        original_text = row['description']\n",
    "        variations = augment_text(original_text, variations_per_sample)\n",
    "        \n",
    "        for text in variations:\n",
    "            augmented_data.append({\n",
    "                'description': text,\n",
    "                'disease': row['disease'],\n",
    "                'is_original': (text == original_text)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(augmented_data)\n",
    "\n",
    "# Create augmented dataset\n",
    "augmented_df = augment_dataset(df, variations_per_sample=10)\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Augmented dataset size: {len(augmented_df)}\")\n",
    "print(f\"Number of variations per sample: {len(augmented_df)/len(df):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f8e6fdd9-47db-4c2c-91a9-04fc62fc159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def syllable_tokenizer(text):\n",
    "    vowels = \"aeiou\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # remove punctuation\n",
    "    words = text.split()\n",
    "    syllables = []\n",
    "\n",
    "    for word in words:\n",
    "        i = 0\n",
    "        w_len = len(word)\n",
    "        word_syllables = []\n",
    "        while i < w_len:\n",
    "            # If it's a vowel, it's a syllable\n",
    "            if word[i] in vowels:\n",
    "                if i + 1 < w_len and word[i + 1] in vowels:\n",
    "                    word_syllables.append(word[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    word_syllables.append(word[i])\n",
    "                    i += 1\n",
    "                continue\n",
    "\n",
    "            # Consonant + Vowel\n",
    "            if i + 1 < w_len and word[i+1] in vowels:\n",
    "                # Check for a nasal+consonant like 'mw', 'ny'\n",
    "                if word[i:i+2] in ['mw', 'ny', 'ng', 'nd', 'mb', 'nj', 'sy', 'sh']:\n",
    "                    if i + 2 < w_len and word[i+2] in vowels:\n",
    "                        word_syllables.append(word[i:i+3])\n",
    "                        i += 3\n",
    "                        continue\n",
    "                word_syllables.append(word[i:i+2])\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            # Consonant + Vowel + Consonant (like mwa)\n",
    "            if i + 2 < w_len and word[i+1] in vowels and word[i+2] not in vowels:\n",
    "                word_syllables.append(word[i:i+3])\n",
    "                i += 3\n",
    "                continue\n",
    "\n",
    "            # Default to one letter (backup case)\n",
    "            word_syllables.append(word[i])\n",
    "            i += 1\n",
    "\n",
    "        syllables.extend(word_syllables)\n",
    "\n",
    "    return syllables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0de606c2-2101-4280-a0df-948e0e9e7676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Syllable tokenization:\n",
      "naumwa kichwa na ninahisi kichwani\n",
      "→ ['na', 'u', 'm', 'wa', 'ki', 'c', 'h', 'wa', 'na', 'ni', 'na', 'hi', 'si', 'ki', 'c', 'h', 'wa', 'ni']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"naumwa kichwa na ninahisi kichwani\"\n",
    "print(\"\\nImproved Syllable tokenization:\")\n",
    "print(test_text)\n",
    "print(\"→\", syllable_tokenizer(test_text))\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=syllable_tokenizer,\n",
    "    lowercase=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f21f47ee-254f-4f11-ad53-51df6814f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 1648\n",
      "Test samples: 412\n"
     ]
    }
   ],
   "source": [
    "# Split augmented data\n",
    "X = augmented_df['description']\n",
    "y = augmented_df['disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82dd8e12-8a1a-4fc9-a3cd-05e091a67586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarxemo/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Malaria       0.81      0.85      0.83       136\n",
      "     Typhoid       0.87      0.82      0.84       127\n",
      "         UTI       0.88      0.87      0.88       149\n",
      "\n",
      "    accuracy                           0.85       412\n",
      "   macro avg       0.85      0.85      0.85       412\n",
      "weighted avg       0.85      0.85      0.85       412\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['swahili_disease_predictor_syllable.pkl']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline with syllable-level features\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', CalibratedClassifierCV(\n",
    "        LinearSVC(\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(pipeline, 'swahili_disease_predictor_syllable.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e729fd-e560-44b2-a74a-59a5740c771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_model = joblib.load('swahili_disease_predictor_syllable.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e24a9042-dbbf-439c-be28-8a26985128e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Examples:\n",
      "\n",
      "Input: 'headache'\n",
      "Syllables: ['he', 'a', 'da', 'c', 'he']\n",
      "Result: Ugonjwa unaotambuliwa: Malaria (uhakiki: 79.8%)\n",
      "Confidence: 0.80\n",
      "\n",
      "Input: 'ninaumwa kichwani'\n",
      "Syllables: ['ni', 'na', 'u', 'm', 'wa', 'ki', 'c', 'h', 'wa', 'ni']\n",
      "Result: Ugonjwa unaotambuliwa: UTI (uhakiki: 64.6%)\n",
      "Confidence: 0.65\n",
      "\n",
      "Input: 'nahisi kizunguzung'\n",
      "Syllables: ['na', 'hi', 'si', 'ki', 'zu', 'n', 'gu', 'zu', 'n', 'g']\n",
      "Result: Ugonjwa unaotambuliwa: Malaria (uhakiki: 97.7%)\n",
      "Confidence: 0.98\n",
      "\n",
      "Input: 'sina njaa leo'\n",
      "Syllables: ['si', 'na', 'n', 'ja', 'a', 'le', 'o']\n",
      "Result: Ugonjwa unaotambuliwa: Typhoid (uhakiki: 80.4%)\n",
      "Confidence: 0.80\n",
      "\n",
      "Input: 'kukojoa kila baada ya dakika tano'\n",
      "Syllables: ['ku', 'ko', 'jo', 'a', 'ki', 'la', 'ba', 'a', 'da', 'ya', 'da', 'ki', 'ka', 'ta', 'no']\n",
      "Result: Ugonjwa unaotambuliwa: UTI (uhakiki: 91.8%)\n",
      "Confidence: 0.92\n"
     ]
    }
   ],
   "source": [
    "def predict_with_confidence(text, threshold=0.6):\n",
    "    \"\"\"Predict disease with syllable pattern recognition\"\"\"\n",
    "    try:\n",
    "        # Get probabilities\n",
    "        proba = disease_model.predict_proba([text])[0]\n",
    "        classes = disease_model.classes_\n",
    "        \n",
    "        # Get top prediction\n",
    "        max_idx = np.argmax(proba)\n",
    "        top_class = classes[max_idx]\n",
    "        top_prob = proba[max_idx]\n",
    "        \n",
    "        # Check confidence threshold\n",
    "        if top_prob >= threshold:\n",
    "            return {\n",
    "                'disease': top_class,\n",
    "                'confidence': float(top_prob),\n",
    "                'message': f\"Ugonjwa unaotambuliwa: {top_class} (uhakiki: {top_prob:.1%})\",\n",
    "                'syllables': syllable_tokenizer(text)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'disease': None,\n",
    "                'confidence': float(top_prob),\n",
    "                'message': \"Haujatambuliwa ugonjwa wowote kutokana na maelezo yako.\",\n",
    "                'syllables': syllable_tokenizer(text)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'disease': None,\n",
    "            'confidence': 0.0,\n",
    "            'message': f\"Hitilafu katika uchambuzi: {str(e)}\",\n",
    "            'syllables': []\n",
    "        }\n",
    "\n",
    "# Test prediction\n",
    "test_phrases = [\n",
    "    \"headache\",\n",
    "    \"ninaumwa kichwani\",\n",
    "    \"nahisi kizunguzung\",\n",
    "    \"sina njaa leo\",\n",
    "    \"kukojoa kila baada ya dakika tano\"\n",
    "]\n",
    "\n",
    "print(\"\\nPrediction Examples:\")\n",
    "for phrase in test_phrases:\n",
    "    result = predict_with_confidence(phrase)\n",
    "    print(f\"\\nInput: '{phrase}'\")\n",
    "    print(f\"Syllables: {result['syllables']}\")\n",
    "    print(f\"Result: {result['message']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab3a9f-3df8-4414-b2b5-52d6deb54459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e27de0-79df-4161-92cf-ce6df9697ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22343521-cc01-4637-865f-fc08ebbbacc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
